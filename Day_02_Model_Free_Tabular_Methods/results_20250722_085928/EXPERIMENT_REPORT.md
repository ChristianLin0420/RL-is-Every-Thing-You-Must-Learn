# SARSA vs Q-Learning: Experimental Results

**Generated on:** 2025-07-22 09:00:07

**Total Experiment Time:** 19.32 seconds (0.3 minutes)

## ğŸ”§ Experiment Configuration

- **Environments:** CliffWalking-v0, FrozenLake-v1
- **Episodes per run:** 1500
- **Random seeds:** [42, 123, 456, 789, 999]
- **Number of runs per algorithm:** 5

## ğŸ“Š Algorithm Overview

### SARSA (State-Action-Reward-State-Action)
- **Type:** On-policy temporal difference learning
- **Update rule:** `Q(s,a) â† Q(s,a) + Î±[r + Î³Q(s',a') - Q(s,a)]`
- **Characteristics:** Uses actual next action selected by policy
- **Behavior:** More cautious, learns from followed policy

### Q-Learning
- **Type:** Off-policy temporal difference learning
- **Update rule:** `Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]`
- **Characteristics:** Uses maximum Q-value for next state
- **Behavior:** More aggressive, learns optimal policy regardless of behavior

## ğŸŒ Results: CliffWalking-v0

### ğŸ“ˆ Performance Summary

| Metric | SARSA | Q-Learning | Difference | Winner |
|--------|--------|------------|------------|--------|
| Final Reward | -15.000 Â± 0.000 | -13.000 Â± 0.000 | +2.000 | Q-Learning |
| Success Rate | 0.000 Â± 0.000 | 0.000 Â± 0.000 | +0.000 | Tie |
| Training Time | 0.301 Â± 0.004 | 0.359 Â± 0.002 | +0.058 | Q-Learning |
| Learning Curve | -15.224 Â± 0.079 | -15.736 Â± 0.502 | -0.512 | SARSA |

### ğŸ” Key Insights

- **Q-Learning outperforms SARSA** in final reward by 2.00
- **CliffWalking environment**: SARSA typically more conservative (avoids cliff), Q-Learning more aggressive (may risk cliff for optimal path)

### ğŸ“Š Generated Visualizations

- **Learning Curves**: `learning_curves_CliffWalking.png`
- **Policy Comparison**: `policy_comparison_CliffWalking.png`
- **Q Value Heatmaps**: `q_value_heatmaps_CliffWalking.png`
- **Summary Dashboard**: `summary_dashboard_CliffWalking.png`

## ğŸŒ Results: FrozenLake-v1

### ğŸ“ˆ Performance Summary

| Metric | SARSA | Q-Learning | Difference | Winner |
|--------|--------|------------|------------|--------|
| Final Reward | 0.000 Â± 0.000 | 0.000 Â± 0.000 | +0.000 | Tie |
| Success Rate | 0.000 Â± 0.000 | 0.000 Â± 0.000 | +0.000 | Tie |
| Training Time | 1.306 Â± 0.018 | 1.730 Â± 0.006 | +0.424 | Q-Learning |
| Learning Curve | 0.000 Â± 0.000 | 0.000 Â± 0.000 | +0.000 | Tie |

### ğŸ” Key Insights

- **Similar performance** between algorithms in final reward
- **FrozenLake environment**: Both algorithms face stochastic transitions, testing robustness to environmental uncertainty

### ğŸ“Š Generated Visualizations

- **Learning Curves**: `learning_curves_FrozenLake.png`
- **Policy Comparison**: `policy_comparison_FrozenLake.png`
- **Q Value Heatmaps**: `q_value_heatmaps_FrozenLake.png`
- **Summary Dashboard**: `summary_dashboard_FrozenLake.png`

## ğŸ¯ Overall Conclusions

### ğŸ† Algorithm Performance Summary

**Q-Learning** shows superior performance across environments.

**Why Q-Learning excels:**
- Off-policy learning allows exploration of optimal actions
- Maximum operator in update rule drives towards optimal policy
- Less conservative, willing to take risks for better long-term rewards

### ğŸ”¬ Theoretical vs Practical Observations

**Theoretical Expectations:**
- Q-Learning should learn optimal policy faster (off-policy)
- SARSA should be more stable and conservative (on-policy)
- Both should converge to optimal policy under ideal conditions

**Practical Results:**
- Convergence speed varies by environment structure
- Exploration strategy (Îµ-greedy) significantly impacts both algorithms
- Environment stochasticity affects learning stability

## ğŸ’¡ Recommendations

### When to use SARSA:
- Safety-critical applications (e.g., robotics, autonomous vehicles)
- Environments with severe penalties for suboptimal actions
- When behavior policy must be close to learned policy

### When to use Q-Learning:
- Pure performance optimization scenarios
- Environments where exploration risks are acceptable
- When learning optimal policy is more important than safe exploration

### Hyperparameter Tuning:
- **Learning rate (Î±)**: Higher for faster learning, lower for stability
- **Epsilon decay**: Slower decay for better exploration, faster for exploitation
- **Discount factor (Î³)**: Higher for long-term planning, lower for immediate rewards

## ğŸš€ Future Directions

- **Function Approximation**: Test with neural networks for larger state spaces
- **Advanced Exploration**: Compare with UCB, Thompson sampling, etc.
- **Double Q-Learning**: Reduce overestimation bias
- **Expected SARSA**: Combine benefits of both approaches
- **Multi-step Methods**: TD(Î») for better credit assignment

---
*Report generated by Day 2 Challenge: Model-Free Tabular Methods*
