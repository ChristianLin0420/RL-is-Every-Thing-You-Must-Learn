# SARSA vs Q-Learning: Experimental Results

**Generated on:** 2025-07-22 09:00:07

**Total Experiment Time:** 19.32 seconds (0.3 minutes)

## 🔧 Experiment Configuration

- **Environments:** CliffWalking-v0, FrozenLake-v1
- **Episodes per run:** 1500
- **Random seeds:** [42, 123, 456, 789, 999]
- **Number of runs per algorithm:** 5

## 📊 Algorithm Overview

### SARSA (State-Action-Reward-State-Action)
- **Type:** On-policy temporal difference learning
- **Update rule:** `Q(s,a) ← Q(s,a) + α[r + γQ(s',a') - Q(s,a)]`
- **Characteristics:** Uses actual next action selected by policy
- **Behavior:** More cautious, learns from followed policy

### Q-Learning
- **Type:** Off-policy temporal difference learning
- **Update rule:** `Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') - Q(s,a)]`
- **Characteristics:** Uses maximum Q-value for next state
- **Behavior:** More aggressive, learns optimal policy regardless of behavior

## 🌍 Results: CliffWalking-v0

### 📈 Performance Summary

| Metric | SARSA | Q-Learning | Difference | Winner |
|--------|--------|------------|------------|--------|
| Final Reward | -15.000 ± 0.000 | -13.000 ± 0.000 | +2.000 | Q-Learning |
| Success Rate | 0.000 ± 0.000 | 0.000 ± 0.000 | +0.000 | Tie |
| Training Time | 0.301 ± 0.004 | 0.359 ± 0.002 | +0.058 | Q-Learning |
| Learning Curve | -15.224 ± 0.079 | -15.736 ± 0.502 | -0.512 | SARSA |

### 🔍 Key Insights

- **Q-Learning outperforms SARSA** in final reward by 2.00
- **CliffWalking environment**: SARSA typically more conservative (avoids cliff), Q-Learning more aggressive (may risk cliff for optimal path)

### 📊 Generated Visualizations

- **Learning Curves**: `learning_curves_CliffWalking.png`
- **Policy Comparison**: `policy_comparison_CliffWalking.png`
- **Q Value Heatmaps**: `q_value_heatmaps_CliffWalking.png`
- **Summary Dashboard**: `summary_dashboard_CliffWalking.png`

## 🌍 Results: FrozenLake-v1

### 📈 Performance Summary

| Metric | SARSA | Q-Learning | Difference | Winner |
|--------|--------|------------|------------|--------|
| Final Reward | 0.000 ± 0.000 | 0.000 ± 0.000 | +0.000 | Tie |
| Success Rate | 0.000 ± 0.000 | 0.000 ± 0.000 | +0.000 | Tie |
| Training Time | 1.306 ± 0.018 | 1.730 ± 0.006 | +0.424 | Q-Learning |
| Learning Curve | 0.000 ± 0.000 | 0.000 ± 0.000 | +0.000 | Tie |

### 🔍 Key Insights

- **Similar performance** between algorithms in final reward
- **FrozenLake environment**: Both algorithms face stochastic transitions, testing robustness to environmental uncertainty

### 📊 Generated Visualizations

- **Learning Curves**: `learning_curves_FrozenLake.png`
- **Policy Comparison**: `policy_comparison_FrozenLake.png`
- **Q Value Heatmaps**: `q_value_heatmaps_FrozenLake.png`
- **Summary Dashboard**: `summary_dashboard_FrozenLake.png`

## 🎯 Overall Conclusions

### 🏆 Algorithm Performance Summary

**Q-Learning** shows superior performance across environments.

**Why Q-Learning excels:**
- Off-policy learning allows exploration of optimal actions
- Maximum operator in update rule drives towards optimal policy
- Less conservative, willing to take risks for better long-term rewards

### 🔬 Theoretical vs Practical Observations

**Theoretical Expectations:**
- Q-Learning should learn optimal policy faster (off-policy)
- SARSA should be more stable and conservative (on-policy)
- Both should converge to optimal policy under ideal conditions

**Practical Results:**
- Convergence speed varies by environment structure
- Exploration strategy (ε-greedy) significantly impacts both algorithms
- Environment stochasticity affects learning stability

## 💡 Recommendations

### When to use SARSA:
- Safety-critical applications (e.g., robotics, autonomous vehicles)
- Environments with severe penalties for suboptimal actions
- When behavior policy must be close to learned policy

### When to use Q-Learning:
- Pure performance optimization scenarios
- Environments where exploration risks are acceptable
- When learning optimal policy is more important than safe exploration

### Hyperparameter Tuning:
- **Learning rate (α)**: Higher for faster learning, lower for stability
- **Epsilon decay**: Slower decay for better exploration, faster for exploitation
- **Discount factor (γ)**: Higher for long-term planning, lower for immediate rewards

## 🚀 Future Directions

- **Function Approximation**: Test with neural networks for larger state spaces
- **Advanced Exploration**: Compare with UCB, Thompson sampling, etc.
- **Double Q-Learning**: Reduce overestimation bias
- **Expected SARSA**: Combine benefits of both approaches
- **Multi-step Methods**: TD(λ) for better credit assignment

---
*Report generated by Day 2 Challenge: Model-Free Tabular Methods*
