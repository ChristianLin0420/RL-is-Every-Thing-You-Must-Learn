# Day 2: Model-Free Tabular Methods

## ğŸ¯ Goal
Master temporal difference learning algorithms that don't require environment models.

## ğŸ“‹ Task
- Implement SARSA (on-policy TD control)
- Implement Q-learning (off-policy TD control)
- Compare performance on CliffWalking or FrozenLake environments
- Analyze exploration vs exploitation tradeoffs

## ğŸ”‘ Key Concepts
- **SARSA**: Q(s,a) â† Q(s,a) + Î±[r + Î³Q(s',a') - Q(s,a)]
- **Q-Learning**: Q(s,a) â† Q(s,a) + Î±[r + Î³max_a'Q(s',a') - Q(s,a)]
- **Îµ-greedy exploration**: Balance between exploitation and exploration
- **Temporal Difference**: Learning from one-step lookahead

## ğŸ“š Learning Objectives
1. Understand difference between on-policy and off-policy learning
2. Implement and compare SARSA vs Q-learning
3. Analyze the role of exploration strategies
4. Visualize learning curves and convergence behavior

## ğŸ› ï¸ Implementation Guidelines
- Use OpenAI Gym's CliffWalking-v0 or FrozenLake-v1
- Implement both algorithms with Îµ-greedy exploration
- Track episode rewards and Q-table evolution
- Create comparison plots and analysis

## ğŸ“– Resources
- Sutton & Barto Chapter 6: Temporal-Difference Learning
- Comparison of TD learning algorithms 