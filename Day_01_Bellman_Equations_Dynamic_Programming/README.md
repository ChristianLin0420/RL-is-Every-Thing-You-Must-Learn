# Day 1: Bellman Equations & Dynamic Programming

## ğŸ¯ Goal
Ensure fluency in key RL concepts and derive fundamental equations for value-based methods.

## ğŸ“‹ Task
- Derive Bellman optimality equations from first principles
- Implement Value Iteration algorithm in a gridworld environment
- Compare convergence properties and computational complexity

## ğŸ”‘ Key Concepts
- **Bellman Optimality Equation**: V*(s) = max_a Î£ P(s'|s,a)[R(s,a,s') + Î³V*(s')]
- **Value Iteration**: Iterative algorithm to find optimal value function
- **Policy Iteration**: Alternative approach using policy evaluation and improvement
- **Dynamic Programming**: Solving complex problems by breaking them into subproblems

## ğŸ“š Learning Objectives
1. Understand the mathematical foundation of value-based RL
2. Implement tabular dynamic programming algorithms
3. Analyze convergence guarantees and computational requirements
4. Visualize value function evolution during learning

## ğŸ› ï¸ Implementation Guidelines
- Create a simple gridworld environment (e.g., 4x4 or 8x8)
- Implement Value Iteration with visualization
- Track and plot value function convergence
- Compare with Policy Iteration if time permits

## ğŸ“– Resources
- Sutton & Barto Chapter 4: Dynamic Programming
- David Silver's RL Course Lecture 3 