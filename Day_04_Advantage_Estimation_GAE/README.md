# Day 4: Advantage Estimation (GAE)

## ğŸ¯ Goal
Understand bias-variance tradeoff in advantage estimation and implement GAE.

## ğŸ“‹ Task
- Implement Generalized Advantage Estimation (GAE)
- Integrate GAE into a policy gradient agent
- Compare different Î» values and their effect on learning
- Analyze bias-variance tradeoff empirically

## ğŸ”‘ Key Concepts
- **Advantage Function**: A(s,a) = Q(s,a) - V(s)
- **GAE**: A^GAE(Î³,Î») = Î£(Î³Î»)^l Î´_{t+l}
- **TD Error**: Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)
- **Bias-Variance Tradeoff**: Î»=0 (high bias, low variance) vs Î»=1 (low bias, high variance)

## ğŸ“š Learning Objectives
1. Understand why advantage estimation is crucial for policy gradients
2. Derive and implement GAE formula
3. Analyze effect of Î» parameter on learning stability
4. Compare with simple advantage estimation methods

## ğŸ› ï¸ Implementation Guidelines
- Extend previous policy gradient implementation
- Implement value function network for baseline
- Experiment with different Î» values (0.0, 0.9, 0.95, 1.0)
- Plot learning curves and advantage estimates

## ğŸ“– Resources
- High-Dimensional Continuous Control Using Generalized Advantage Estimation (Schulman et al.)
- Actor-Critic methods with GAE 