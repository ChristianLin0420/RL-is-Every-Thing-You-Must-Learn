# üìò Day 4: Advantage Estimation (GAE)

Welcome to Day 4 of the Advanced RL Challenge! Today we implement **Generalized Advantage Estimation (GAE)** and explore the crucial **bias-variance tradeoff** in policy gradients.

## üéØ **Objectives**

- ‚úÖ **Understand and derive** GAE from first principles
- ‚úÖ **Implement Actor-Critic** with configurable GAE-Œª 
- ‚úÖ **Analyze bias-variance tradeoff** across different Œª values
- ‚úÖ **Train on CartPole and LunarLander** environments
- ‚úÖ **Visualize** learning curves, advantage distributions, and Œª effects
- ‚úÖ **Compare** GAE with other advantage estimation methods

## üß† **Theoretical Foundation**

### **The Advantage Function**

The advantage function quantifies how much better an action is compared to the average:

$$A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$$

This provides a **zero-centered baseline** that reduces gradient variance while maintaining unbiased estimates.

### **Generalized Advantage Estimation (GAE-Œª)**

GAE-Œª provides a unified framework for advantage estimation by combining multiple n-step returns:

$$\hat{A}_t^{GAE(\gamma,\lambda)} = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}$$

where the **TD error** is:
$$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$$

#### **Key Insight: Bias-Variance Tradeoff**

- **Œª = 0**: High bias, low variance (1-step TD)
  - $\hat{A}_t = \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$
  - Fast convergence, but biased if value function is inaccurate

- **Œª = 1**: Low bias, high variance (Monte Carlo)
  - $\hat{A}_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k} - V(s_t)$
  - Unbiased but high variance, slower convergence

- **Œª ‚àà (0,1)**: Balanced tradeoff
  - **Œª = 0.95** often works well in practice

### **GAE Derivation**

Starting from the recursive definition:
$$\hat{A}_t^{GAE(\gamma,\lambda)} = \delta_t + \gamma\lambda \hat{A}_{t+1}^{GAE(\gamma,\lambda)}$$

Expanding backwards:
$$\hat{A}_t^{GAE(\gamma,\lambda)} = \delta_t + \gamma\lambda(\delta_{t+1} + \gamma\lambda\hat{A}_{t+2}^{GAE(\gamma,\lambda)})$$

This gives us the **exponentially-weighted sum**:
$$\hat{A}_t^{GAE(\gamma,\lambda)} = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}$$

## üìÅ **Project Structure**

```
Day_04_Advantage_Estimation_GAE/
‚îú‚îÄ‚îÄ env_wrapper.py         # Environment standardization and tracking
‚îú‚îÄ‚îÄ model.py               # Actor-Critic network architectures  
‚îú‚îÄ‚îÄ gae_utils.py           # GAE computation and bias-variance analysis
‚îú‚îÄ‚îÄ train.py               # Training loops and experiment management
‚îú‚îÄ‚îÄ visualize.py           # Comprehensive visualization tools
‚îú‚îÄ‚îÄ main.py                # Command-line interface
‚îî‚îÄ‚îÄ README.md              # This documentation
```

## üöÄ **Usage**

### **Basic GAE Training**

```bash
# Train Actor-Critic with GAE on CartPole
python main.py --mode basic --env CartPole-v1 --episodes 300

# Try different Œª values
python main.py --mode basic --lambda-gae 0.9 --episodes 300
python main.py --mode basic --lambda-gae 1.0 --episodes 300  # Monte Carlo

# Train on LunarLander (more challenging)
python main.py --mode basic --env LunarLander-v2 --episodes 500 --lr 1e-3
```

### **Lambda Sweep Experiment**

```bash
# Quick sweep of key Œª values
python main.py --mode sweep --quick --episodes 200

# Comprehensive sweep with multiple seeds
python main.py --mode sweep --episodes 300 --seeds 5

# Environment-specific sweep
python main.py --mode sweep --env LunarLander-v2 --episodes 400 --seeds 3
```

### **Bias-Variance Analysis**

```bash
# Detailed analysis of advantage estimation methods
python main.py --mode analysis --num-trajectories 100

# Quick analysis
python main.py --mode analysis --num-trajectories 30 --env CartPole-v1
```

### **Interactive Monitoring**

```bash
# Interactive training visualization
python main.py --mode interactive --episodes 300 --lambda-gae 0.95

# Monitor previous training results
python main.py --mode interactive --load-results results/basic_training_20231201_143022.pkl
```

### **Model Evaluation**

```bash
# Evaluate trained model
python main.py --mode evaluate --model-path models/gae_basic_CartPole-v1_20231201_143022.pth --eval-episodes 10

# Evaluate with rendering
python main.py --mode evaluate --model-path models/gae_basic_CartPole-v1_20231201_143022.pth --render
```

## üìä **Expected Results**

### **Environment Performance**

#### **CartPole-v1**
- **State Space**: 4D continuous (position, velocity, angle, angular velocity)
- **Action Space**: 2 discrete actions (left, right)
- **Success Criteria**: Average reward > 475 over 100 episodes

**Typical Results**:
- **Œª = 0.0**: Converges in ~150 episodes, some instability
- **Œª = 0.95**: Converges in ~100 episodes, stable learning
- **Œª = 1.0**: Converges in ~200 episodes, high variance

#### **LunarLander-v2**
- **State Space**: 8D continuous (position, velocity, angle, contact sensors)
- **Action Space**: 4 discrete actions (engines)
- **Success Criteria**: Average reward > 200 over 100 episodes

**Typical Results**:
- **Œª = 0.95**: Converges in ~300-400 episodes
- **Œª = 1.0**: Slower convergence, higher variance
- **Œª = 0.0**: Faster early learning, may plateau

### **Lambda Sweep Analysis**

#### **Performance vs Œª**
```
Œª=0.00: 420.5 ¬± 25.3  (High bias, low variance)
Œª=0.30: 445.2 ¬± 18.7  (Balanced)
Œª=0.50: 462.1 ¬± 15.2  (Good balance)
Œª=0.70: 475.8 ¬± 12.4  (Optimal for CartPole)
Œª=0.90: 478.2 ¬± 14.1  (Near optimal)
Œª=0.95: 480.1 ¬± 13.8  (Excellent)
Œª=0.99: 476.3 ¬± 16.5  (Slight increase in variance)
Œª=1.00: 465.7 ¬± 22.1  (High variance)
```

#### **Key Insights**
1. **Sweet Spot**: Œª ‚àà [0.9, 0.95] typically optimal
2. **Environment Dependence**: Optimal Œª varies by environment complexity
3. **Convergence Speed**: Lower Œª converges faster initially
4. **Stability**: Higher Œª more stable but slower convergence

## üîç **Implementation Details**

### **Actor-Critic Architecture**

#### **Actor Network (Policy)**
```python
Input: State (obs_dim,)
Hidden: (64, 64) with Tanh activation
Output: Action logits (discrete) or (mean, log_std) (continuous)
```

#### **Critic Network (Value)**
```python
Input: State (obs_dim,)
Hidden: (64, 64) with Tanh activation  
Output: State value (1,)
```

#### **Training Configuration**
- **Learning Rate**: 3e-4 (both actor and critic)
- **Optimizer**: Adam
- **Gradient Clipping**: Max norm 0.5
- **Value Loss Coefficient**: 0.5
- **Entropy Coefficient**: 0.01

### **GAE Computation**

```python
def compute_gae(rewards, values, dones, next_value, gamma=0.99, lambda_gae=0.95):
    """
    Compute GAE advantages using reverse iteration.
    
    advantages[t] = delta[t] + gamma * lambda * advantages[t+1] * (1 - done[t])
    returns[t] = advantages[t] + values[t]
    """
    T = len(rewards)
    advantages = torch.zeros_like(rewards)
    
    # Compute TD errors
    deltas = torch.zeros_like(rewards)
    for t in range(T):
        next_val = next_value if t == T-1 else values[t+1]
        next_val = next_val if not dones[t] else 0.0
        deltas[t] = rewards[t] + gamma * next_val - values[t]
    
    # Compute GAE advantages
    gae = 0.0
    for t in reversed(range(T)):
        gae = deltas[t] + gamma * lambda_gae * gae * (1 - dones[t])
        advantages[t] = gae
    
    returns = advantages + values
    return advantages, returns
```

## üìà **Generated Outputs**

### **Training Visualizations**

#### **`figures/gae_training_progress_*.png`**
6-panel training dashboard:
1. **Episode Rewards**: Raw and smoothed learning curves
2. **Training Losses**: Policy, value, and total loss evolution
3. **Gradients & Exploration**: Gradient norms and policy entropy
4. **GAE Advantages**: Mean and standard deviation over time
5. **Returns vs Values**: Comparison of predicted and target values
6. **Training Efficiency**: Training time and episode lengths

#### **`figures/lambda_sweep_analysis_*.png`**
4-panel Œª sweep analysis:
1. **Performance vs Œª**: Final reward with error bars
2. **Convergence Speed**: Episodes to reach 80% of final performance
3. **Advantage Variance**: How Œª affects gradient variance
4. **Learning Curves**: Comparison of different Œª values

#### **`figures/bias_variance_analysis_*.png`**
4-panel bias-variance comparison:
1. **Advantage Distributions**: Box plots for different methods
2. **Variance vs Œª**: Clear visualization of bias-variance tradeoff
3. **Advantage Evolution**: Time series for sample episode
4. **Statistical Summary**: Comprehensive statistics table

### **Interactive Visualizations**

#### **Interactive Training Monitor**
- **Episode Slider**: Navigate through training progress
- **Real-time Metrics**: Current performance statistics
- **Dynamic Plots**: Updates based on selected episode range
- **GAE Statistics**: Live Œª and advantage analysis

## üß™ **Experiments and Analysis**

### **Experiment 1: Optimal Œª Discovery**

**Hypothesis**: Œª ‚àà [0.9, 0.95] provides best performance balance

```bash
python main.py --mode sweep --env CartPole-v1 --episodes 300 --seeds 5
```

**Expected Results**:
- Œª = 0.95 achieves highest mean performance
- Œª = 1.0 shows highest variance
- Œª = 0.0 converges fastest but plateaus early

### **Experiment 2: Environment Complexity Effect**

**Hypothesis**: Complex environments benefit more from higher Œª

```bash
# Simple environment
python main.py --mode sweep --env CartPole-v1 --quick

# Complex environment  
python main.py --mode sweep --env LunarLander-v2 --quick
```

**Expected Results**:
- CartPole: Optimal Œª ‚âà 0.9
- LunarLander: Optimal Œª ‚âà 0.95-0.99

### **Experiment 3: Bias-Variance Decomposition**

**Hypothesis**: GAE reduces variance compared to Monte Carlo

```bash
python main.py --mode analysis --num-trajectories 100 --env CartPole-v1
```

**Expected Results**:
- Monte Carlo (Œª=1.0): High variance
- GAE (Œª=0.95): Balanced bias-variance
- TD (Œª=0.0): Low variance, potential bias

## üí° **Key Insights**

### **1. GAE Benefits**
- **Unified Framework**: Single parameter controls bias-variance tradeoff
- **Stable Learning**: Reduced gradient variance leads to more stable training
- **Environment Adaptive**: Can tune Œª for different environments

### **2. Practical Guidelines**
- **Start with Œª=0.95**: Good default for most environments
- **Lower Œª for simple envs**: CartPole works well with Œª=0.9
- **Higher Œª for complex envs**: LunarLander benefits from Œª=0.95-0.99
- **Monitor variance**: If training is unstable, reduce Œª

### **3. Theoretical Understanding**
- **Bias comes from value function errors**: More accurate critic allows higher Œª
- **Variance comes from trajectory noise**: Longer rollouts increase variance
- **GAE provides smooth interpolation**: Between biased TD and unbiased MC

### **4. Implementation Tips**
- **Normalize advantages**: Helps with optimization stability
- **Clip gradients**: Prevents exploding gradients with high Œª
- **Tune value loss coefficient**: Balance actor and critic learning

## üèÜ **Key Takeaways**

1. **GAE is fundamental** for modern policy gradient methods
2. **Œª parameter crucially affects** learning speed and stability  
3. **Bias-variance tradeoff** is central to RL algorithm design
4. **Environment complexity** influences optimal Œª choice
5. **Proper advantage estimation** dramatically improves policy gradients
6. **Interactive analysis tools** provide deep insights into algorithm behavior

## üìö **References**

1. **Schulman, J., et al. (2016)**. High-dimensional continuous control using generalized advantage estimation. *ICLR*.

2. **Mnih, V., et al. (2016)**. Asynchronous methods for deep reinforcement learning. *ICML*.

3. **Sutton, R. S., & Barto, A. G. (2018)**. Reinforcement learning: An introduction. MIT press.

---

**üéØ Ready to master advantage estimation? Start with the lambda sweep experiment!**

```bash
python main.py --mode sweep --env CartPole-v1 --episodes 300 --seeds 3
```

**üîç Want to understand the theory deeper? Run the bias-variance analysis:**

```bash
python main.py --mode analysis --num-trajectories 50 --env CartPole-v1
``` 