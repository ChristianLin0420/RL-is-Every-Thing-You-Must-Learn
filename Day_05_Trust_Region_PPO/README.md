# Day 5: Trust Region & PPO

## ğŸ¯ Goal
Derive TRPO's constraint and implement PPO for stable policy updates.

## ğŸ“‹ Task
- Derive Trust Region Policy Optimization (TRPO) constraint
- Implement Proximal Policy Optimization (PPO) from scratch
- Test on continuous control environments (e.g., Pendulum, MuJoCo)
- Compare clipped vs KL-penalty versions

## ğŸ”‘ Key Concepts
- **Trust Region**: Constrain policy updates to prevent performance collapse
- **KL Divergence**: KL(Ï€_old || Ï€_new) â‰¤ Î´
- **PPO Clipping**: clip(r_t(Î¸), 1-Îµ, 1+Îµ) where r_t(Î¸) = Ï€(a|s)/Ï€_old(a|s)
- **Surrogate Objective**: Conservative policy improvement

## ğŸ“š Learning Objectives
1. Understand motivation for trust region methods
2. Derive mathematical foundation of TRPO
3. Implement PPO as practical approximation to TRPO
4. Analyze clipping mechanism and its effects

## ğŸ› ï¸ Implementation Guidelines
- Use continuous control environment (Pendulum-v1)
- Implement both actor and critic networks
- Use GAE from previous day for advantage estimation
- Track KL divergence and clipping statistics

## ğŸ“– Resources
- Trust Region Policy Optimization (Schulman et al.)
- Proximal Policy Optimization Algorithms (Schulman et al.) 